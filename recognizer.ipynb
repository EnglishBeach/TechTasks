{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "plt.rcParams['figure.subplot.left'] = 0.1\n",
    "plt.rcParams['figure.subplot.right'] = 0.99\n",
    "plt.rcParams['figure.subplot.top'] = 0.97\n",
    "plt.rcParams['figure.subplot.bottom'] = 0.05\n",
    "plt.rcParams['figure.subplot.hspace'] = 0.3\n",
    "matplotlib.rc(\"image\", cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1664f46aa10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_real= cv2.imread(\n",
    "r\"D:\\WORKS\\TechTasks\\DocData\\2.jpg\"\n",
    ")\n",
    "plt.imshow(img_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale_image(image: np.ndarray, max_size=2048)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Downscale image\n",
    "\n",
    "    :param image: Input image\n",
    "    :param max_size: Maxi size, defaults to 2048\n",
    "    :return: Downscaled image\n",
    "    \"\"\"\n",
    "    x, y = image.shape\n",
    "    max_dimention = max(x, y)\n",
    "\n",
    "    if max_dimention <= max_size:\n",
    "        return image\n",
    "\n",
    "    scale = max_size / max_dimention\n",
    "    return cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1664a716050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray = cv2.cvtColor(img_real, cv2.COLOR_BGR2GRAY)\n",
    "img_gray = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "img_gray = downscale_image(img_gray)\n",
    "plt.imshow(img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find text boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rank_filter\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class BoxProperties(NamedTuple):\n",
    "    \"\"\"Box parameters: x,y dx,dy\"\"\"\n",
    "\n",
    "    x: int\n",
    "    y: int\n",
    "    dx: int\n",
    "    dy: int\n",
    "\n",
    "\n",
    "class AttentionArea:\n",
    "    \"\"\"Find another blocks around attention area\"\"\"\n",
    "\n",
    "    border_scale = 0.05\n",
    "    \"\"\"Border thickness proporitonal image size\"\"\"\n",
    "\n",
    "    def __init__(self, attention_mask: np.ndarray, image: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Create attention area around attention_mask\n",
    "\n",
    "        :param attention_mask: Used mask to look around\n",
    "        :param image: Full image\n",
    "        \"\"\"\n",
    "        y_full, x_full = image.shape\n",
    "        y_border = int(y_full * self.border_scale)\n",
    "        x_border = int(x_full * self.border_scale)\n",
    "\n",
    "        non_zero_coords = cv2.findNonZero(attention_mask)\n",
    "        x, y, dx, dy = cv2.boundingRect(non_zero_coords)\n",
    "\n",
    "        window_slice = (\n",
    "            slice(np.maximum(y - y_border, 0), y + dy + y_border),\n",
    "            slice(np.maximum(x - x_border, 0), x + dx + x_border),\n",
    "        )\n",
    "        window = np.zeros_like(image)\n",
    "        window[window_slice] = image[window_slice]\n",
    "\n",
    "        self.focus_mask = attention_mask\n",
    "        self.window_mask = window\n",
    "\n",
    "        self.focus_box = BoxProperties(x, y, dx, dy)\n",
    "        self.window_box = BoxProperties(\n",
    "            np.maximum(x - x_border, 0),\n",
    "            np.maximum(y - y_border, 0),\n",
    "            dx + 2 * x_border,\n",
    "            dy + 2 * y_border,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def cropped_masks(self) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return cropped window and attention areas\"\"\"\n",
    "\n",
    "        window_slice = (\n",
    "            slice(self.window_box.y, self.window_box.y + self.window_box.dy),\n",
    "            slice(self.window_box.x, self.window_box.x + self.window_box.dx),\n",
    "        )\n",
    "        return self.focus_mask[window_slice].copy(), self.window_mask[window_slice].copy()\n",
    "\n",
    "    @property\n",
    "    def F1_metric(self) -> float:\n",
    "        \"\"\"F1 metric for a clasterisation quality\"\"\"\n",
    "\n",
    "        attention, window = self.cropped_masks\n",
    "        attention[attention != 0] = 1\n",
    "        window[window != 0] = 1\n",
    "\n",
    "        dy, dx = window.shape\n",
    "\n",
    "        tp = np.count_nonzero(attention)\n",
    "        fp = dx * dy\n",
    "        fn = np.count_nonzero(window - attention)\n",
    "        return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    @property\n",
    "    def periphery_types(self):\n",
    "        return set(np.unique(self.window_mask)) - set(np.unique(self.focus_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watch(attention_mask: np.ndarray, image: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Get attention area around attention_mask\n",
    "\n",
    "#     :param attention_mask: Used mask to look around\n",
    "#     :param image: Full image\n",
    "#     :return: Attention window and parametrs for it (what we see and where need to see)\n",
    "#     \"\"\"\n",
    "\n",
    "#     border_scale = 0.05\n",
    "#     y_full, x_full = image.shape\n",
    "#     y_border = int(y_full * border_scale)\n",
    "#     x_border = int(x_full * border_scale)\n",
    "#     box = Box(attention_mask)\n",
    "#     # x, y, dx, dy = cv2.boundingRect(non_zero_coords)\n",
    "#     window_slice = (\n",
    "#         slice(box.y - y_border, box.y + box.dy + y_border),\n",
    "#         slice(box.x - x_border, box.x + box.dx + x_border),\n",
    "#     )\n",
    "#     window = np.zeros_like(image)\n",
    "#     window[window_slice] = image[window_slice]\n",
    "#     return window, box\n",
    "\n",
    "\n",
    "# def get_F_metric(mask: np.ndarray, window: np.ndarray):\n",
    "#     mask = mask.copy()\n",
    "#     mask[mask != 0] = 1\n",
    "#     window = window.copy()\n",
    "#     window[window != 0] = 1\n",
    "\n",
    "#     dy, dx = window.shape\n",
    "\n",
    "#     tp = np.count_nonzero(mask)\n",
    "#     fp = dx * dy\n",
    "#     fn = np.count_nonzero(window - mask)\n",
    "#     return 2 * tp / (2 * tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(NamedTuple):\n",
    "    \"\"\"Paragraph mask on picture with information\"\"\"\n",
    "\n",
    "    type: int\n",
    "    \"\"\"Mask order number\"\"\"\n",
    "\n",
    "    power: int\n",
    "    \"\"\"Nonzero pixels count Number \"\"\"\n",
    "\n",
    "    mask: np.ndarray\n",
    "    \"\"\"Space mask\"\"\"\n",
    "\n",
    "\n",
    "def get_blocks_map(image: np.ndarray) -> list[Block]:\n",
    "    \"\"\"\n",
    "    Return separate text area masks on image\n",
    "\n",
    "    :param image: Image to analyse separate text blocks\n",
    "    :return: List of text area masks in the order of their power\n",
    "    \"\"\"\n",
    "    max_rectangles = 20\n",
    "\n",
    "    img = cv2.GaussianBlur(image, (5, 5), 5)\n",
    "    img = cv2.Canny(img, 100, 200)\n",
    "    img = rank_filter(img, rank=5, size=3)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    n_rectangles = max_rectangles + 1\n",
    "    while n_rectangles > max_rectangles:\n",
    "        img = cv2.dilate(img, kernel, iterations=1)\n",
    "        n_rectangles, block_map = cv2.connectedComponents(img)\n",
    "        n_rectangles -= 1\n",
    "    return sorted(\n",
    "        [\n",
    "            Block(\n",
    "                i,\n",
    "                np.count_nonzero((block_map == i) * 1),\n",
    "                np.uint8(block_map == i)*i,\n",
    "            )\n",
    "            for i in range(1, n_rectangles)\n",
    "        ],\n",
    "        key=lambda x: x.power,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = img_gray\n",
    "# a = cv2.GaussianBlur(a, (5, 5), 5)\n",
    "# a = cv2.Canny(a, 100, 200)\n",
    "# a = rank_filter(a, rank=5, size=3)\n",
    "# kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "# n_rectangles = 1000\n",
    "# while n_rectangles > 20:\n",
    "#     a = cv2.dilate(a, kernel, iterations=1)\n",
    "#     n_rectangles, block_map = cv2.connectedComponents(a)\n",
    "#     n_rectangles -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_map = get_blocks_map(img_gray)\n",
    "img_block = sum([i.mask for i in block_map])\n",
    "\n",
    "text_clasters = []\n",
    "while len(block_map):\n",
    "    central_block = block_map.pop()\n",
    "    area = AttentionArea(central_block.mask, img_block)\n",
    "    start_F1 = area.F1_metric\n",
    "\n",
    "    periphery_blocks = [block for block in block_map if block.type in area.periphery_types][::-1]\n",
    "\n",
    "    for periphery in periphery_blocks:\n",
    "        if AttentionArea(area.focus_mask + periphery.mask, img_block).F1_metric < 0.9 * start_F1:\n",
    "            continue\n",
    "\n",
    "        area.focus_mask += periphery.mask\n",
    "        block_map.remove(periphery)\n",
    "    text_clasters.append(area.focus_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x166734068f0>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(text_clasters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16672f60f40>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(a.focus_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Box' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ranged_blocks):\n\u001b[0;32m      3\u001b[0m     focused \u001b[38;5;241m=\u001b[39m ranged_blocks\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 4\u001b[0m     window, size \u001b[38;5;241m=\u001b[39m \u001b[43mwatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfocused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     start_F1 \u001b[38;5;241m=\u001b[39m get_F_metric(focused, window, size)\n\u001b[0;32m      7\u001b[0m     watched_blocks \u001b[38;5;241m=\u001b[39m [block \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m ranged_blocks \u001b[38;5;28;01mif\u001b[39;00m block\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(window))]\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mwatch\u001b[1;34m(attention_mask, image)\u001b[0m\n\u001b[0;32m     12\u001b[0m y_border \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(y_full \u001b[38;5;241m*\u001b[39m border_scale)\n\u001b[0;32m     13\u001b[0m x_border \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x_full \u001b[38;5;241m*\u001b[39m border_scale)\n\u001b[1;32m---> 14\u001b[0m box \u001b[38;5;241m=\u001b[39m \u001b[43mBox\u001b[49m(attention_mask)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# x, y, dx, dy = cv2.boundingRect(non_zero_coords)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m window_slice \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mslice\u001b[39m(box\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m-\u001b[39m y_border, box\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m+\u001b[39m box\u001b[38;5;241m.\u001b[39mdy \u001b[38;5;241m+\u001b[39m y_border),\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mslice\u001b[39m(box\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m-\u001b[39m x_border, box\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m+\u001b[39m box\u001b[38;5;241m.\u001b[39mdx \u001b[38;5;241m+\u001b[39m x_border),\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Box' is not defined"
     ]
    }
   ],
   "source": [
    "paragraphs =[]\n",
    "while len(ranged_blocks):\n",
    "    focused = ranged_blocks.pop().mask.copy()\n",
    "    window_mask, size = watch(focused, img_block)\n",
    "    start_F1 = get_F_metric(focused, window_mask, size)\n",
    "\n",
    "    watched_blocks = [block for block in ranged_blocks if block.i in set(np.unique(window_mask))]\n",
    "\n",
    "    while len(watched_blocks):\n",
    "        add_block = watched_blocks.pop()\n",
    "        focused_new = focused + add_block.mask\n",
    "        window_mask, size = watch(focused_new, img_block)\n",
    "        F1 = get_F_metric(focused_new, window_mask, size)\n",
    "        if F1 >= start_F1 * 0.9:\n",
    "            start_F1 = F1\n",
    "            focused = focused_new\n",
    "            ranged_blocks.remove(add_block)\n",
    "    paragraphs.append(focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(focused_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sum([paragraphs[i] * (i+1) for i in range(len(paragraphs))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(paragraphs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_area_rect(image):\n",
    "    non_zero_coords = cv2.findNonZero(image)\n",
    "    return cv2.minAreaRect(non_zero_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _open_transform(image, kernel_size):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    filtered_image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "    return filtered_image\n",
    "\n",
    "\n",
    "# TODO: find deriv from generator\n",
    "def _hightlight_text(image):\n",
    "    quality = np.array([np.count_nonzero(_open_transform(image, i)) for i in range(1, 10)])\n",
    "    d_quality = np.diff(quality)\n",
    "    best_kernel_size = np.where(d_quality == min(d_quality))[0][0] + 1\n",
    "\n",
    "    return _open_transform(image, best_kernel_size)\n",
    "\n",
    "\n",
    "scale = 1\n",
    "img = paragraphs[0]\n",
    "\n",
    "# non_zero_coords = cv2.findNonZero(_hightlight_text(img))\n",
    "# box = cv2.minAreaRect(non_zero_coords)\n",
    "# angle = box[2]\n",
    "\n",
    "# h, w = gray_image.shape\n",
    "# center = (w / 2, h / 2)\n",
    "# # FIXME: check another rotations\n",
    "# rotate_M = cv2.getRotationMatrix2D(center, 45 - np.abs(np.abs(angle) - 45), scale)\n",
    "# aligned_image = cv2.warpAffine(\n",
    "#     gray_image.copy(), rotate_M, (w, h), cv2.INTER_CUBIC, cv2.BORDER_REPLICATE\n",
    "# )\n",
    "\n",
    "# plt.imshow(aligned_image)\n",
    "\n",
    "# x, y, dx, dy = cv2.boundingRect(cv2.findNonZero(img))\n",
    "# plt.imshow(img[y:y+dy,x:x+dx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "mask2 = cv2.dilate(img, kernel, iterations=5)\n",
    "img2 = cv2.bitwise_and(img_gray, img_gray, mask=mask2)\n",
    "plt.imshow(img2)\n",
    "# plt.imshow(mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(lang_list=['ru'],gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.readtext(img2, width_ths=0.7,height_ths=0.7,rotation_info =list(range(0,270)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agat_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
